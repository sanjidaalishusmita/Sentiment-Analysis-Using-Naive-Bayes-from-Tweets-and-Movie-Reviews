{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9171c180",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc7eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed81e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b5eee",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43e905b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment                                      SentimentText\n",
       "0           1          0                       is so sad for my APL frie...\n",
       "1           2          0                     I missed the New Moon trail...\n",
       "2           3          1                            omg its already 7:30 :O\n",
       "3           4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4           5          0           i think mi bf is cheating on me!!!   ...\n",
       "...       ...        ...                                                ...\n",
       "99984   99996          0  @Cupcake  seems like a repeating problem   hop...\n",
       "99985   99997          1  @cupcake__ arrrr we both replied to each other...\n",
       "99986   99998          0                     @CuPcAkE_2120 ya i thought so \n",
       "99987   99999          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
       "99988  100000          1                    @cupcake_kayla haha yes you do \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "twitter_data = pd.read_csv('twitter.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79b61ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.46321095320485"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(twitter_data['Sentiment'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8830f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(twitter_data['SentimentText'], \n",
    "                                                    twitter_data['Sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bc9ec",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7add7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB = MultinomialNB(alpha = 0.1)\n",
    "clfrNB.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973ef094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7354188335066806"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337dc560",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.7 GiB for an array with shape (74991, 87195) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15584/1461483369.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Create a DataFrame(Sparse) with all the scores and get the max of each row to get the sorted Series. {My Note}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mSeries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras-gpu-test\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras-gpu-test\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.7 GiB for an array with shape (74991, 87195) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit & Transform X_train using Tfidf Vectorizer & get the feature names. {My Note}\n",
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "    \n",
    "# Sort index to get the top feature names. {My Note}\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "Sindex = feature_names[sorted_tfidf_index[:20]]\n",
    "Lindex = feature_names[sorted_tfidf_index[:-21:-1]]\n",
    "    \n",
    "# Create a DataFrame(Sparse) with all the scores and get the max of each row to get the sorted Series. {My Note}\n",
    "df = pd.DataFrame(X_train_vectorized.toarray(), columns = vect.get_feature_names_out())\n",
    "Series = sorted(df.max())\n",
    "    \n",
    "# Sort the smallest tf-idfs based on Score and then Alphabetically. {My Note}\n",
    "Sdf = pd.DataFrame(Series[:20],index = Sindex)\n",
    "Sdf = Sdf.reset_index()\n",
    "Sdf = Sdf.sort_values([0, \"index\"], ascending = (True, True))\n",
    "Sdf = Sdf.set_index('index')\n",
    "Sseries = pd.Series(Sdf[0], index = Sdf.index)\n",
    "# Sort the largest tf-idfs based on Score and then Alphabetically. {My Note}\n",
    "Ldf = pd.DataFrame(Series[:-21:-1],index = Lindex)\n",
    "Ldf = Ldf.reset_index()\n",
    "Ldf = Ldf.sort_values([0, \"index\"], ascending = (False, True))\n",
    "Ldf = Ldf.set_index('index')\n",
    "Lseries = pd.Series(Ldf[0], index = Ldf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41589510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "trivialboring                                                               0.990542\n",
       "pokemon                                                                     0.926929\n",
       "ghoulies                                                                    0.913663\n",
       "robot                                                                       0.890573\n",
       "blahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah    0.883381\n",
       "nr                                                                          0.879292\n",
       "esperanto                                                                   0.864549\n",
       "uzumakis                                                                    0.859902\n",
       "ernest                                                                      0.859825\n",
       "bad                                                                         0.851258\n",
       "wei                                                                         0.843160\n",
       "darkman                                                                     0.838596\n",
       "primary                                                                     0.833850\n",
       "lupin                                                                       0.831662\n",
       "cycle                                                                       0.829345\n",
       "jokes                                                                       0.821357\n",
       "akasha                                                                      0.819173\n",
       "shemp                                                                       0.818947\n",
       "botched                                                                     0.816170\n",
       "steve                                                                       0.816081\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f51c58e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "bishoff          0.015378\n",
       "brawled          0.015378\n",
       "chokeslammed     0.015378\n",
       "clotheslining    0.015378\n",
       "crossface        0.015378\n",
       "dudleys          0.015378\n",
       "ganged           0.015378\n",
       "gloated          0.015378\n",
       "goaded           0.015378\n",
       "hurracanrana     0.015378\n",
       "nwo              0.015378\n",
       "powerbomb        0.015378\n",
       "riksihi          0.015378\n",
       "rollup           0.015378\n",
       "somersaulted     0.015378\n",
       "somersaulting    0.015378\n",
       "sprinted         0.015378\n",
       "superkicked      0.015378\n",
       "suplexing        0.015378\n",
       "turnbuckles      0.015378\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84bcebc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7215777262180975"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB = MultinomialNB(alpha = 0.1)\n",
    "clfrNB.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029903c0",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f1d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB2 = BernoulliNB(alpha=0.1)\n",
    "clfrNB2.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB2.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e16633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311784942795424"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6da8695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311784942795424"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit & Transform X_train using Tfidf Vectorizer & get the feature names. {My Note}\n",
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB2 = BernoulliNB(alpha = 0.1)\n",
    "clfrNB2.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB2.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503492e",
   "metadata": {},
   "source": [
    "## Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73dce984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7572605808464677"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB3 = ComplementNB()\n",
    "clfrNB3.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB3.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e51f098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7524201936154893"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "clfrNB3 = ComplementNB()\n",
    "clfrNB3.fit(X_train_vectorized, Y_train)\n",
    "predicted_labels = clfrNB3.predict(X_test_vectorized)\n",
    "score = accuracy_score(Y_test, predicted_labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
